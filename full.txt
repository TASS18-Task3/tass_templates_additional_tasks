eHealth-KD: eHealth Knowledge Discovery - Descubrimiento de conocimiento en Salud


Description of the evaluation task
Natural Language Processing (NLP) methods are increasingly being used to mine knowledge from unstructured health texts. Recent advances in health text processing techniques are encouraging researchers and medical domain experts to go beyond  just reading the information included in published texts (e.g. academic manuscripts, clinical reports, etc.) and structured questionnaires, to discover new knowledge by mining health contents. This has allowed other perspectives to surface that were not previously  available.
Over the years many eHealth challenges have taken place, which have attempted  to identify, classify, extract and link knowledge, such  as Semevals, CLEF campaigns and others [1].
Inspired by previous NLP shared tasks like “Semeval-2017 Task 10: ScienceIE[1]” and  research lines like Teleologies [2], both not specifically focussed on  the health area, eHealth-KD proposes modelling the human language in a scenario in which Spanish electronic health documents could be machine readable from a semantic point of view. With this task, we expect to encourage the development of software technologies to automatically extract a large variety of knowledge from eHealth documents written in the Spanish Language.
The documents used as corpus have been taken from MedlinePlus and manually processed to make them fit for the task. Additional details are provided at the end of this document.
To achieve this purpose, three subtasks are presented:
1. Identification of keyphrases
2. Classification of keyphrases
3. Setting semantic relationships
Subtask A: Identification of keyphrases


Given a list of eHealth documents written in Spanish, the goal of this subtask is to identify all the keyphrases per document.


Input: a list of documents in plain text is given. Each document is named input_<topic>.txt where topic is a keyword such as “asma”, “cancer”, etc., related to the topic of the document. These topic labels are only used in this subtask to relate input files to output files. A description document for the corpus is provided which explains the source of each of the input files.


An example input file containing plain text is given below. No preprocessing— tokenizing or splitting into sentences— is performed on the text. All  inputs are encoded into UTF-8.




Subtask A input example:
* plain text document:  file input_asma.txt
El asma afecta las vías respiratorias. Los pulmones se hinchan. El asma es una enfermedad. Un ataque de asma se produce cuando los síntomas empeoran. Los síntomas y el tratamiento dependen del tipo de cáncer y de lo avanzada que esté la enfermedad.


Output: a list of documents, corresponding to the input files, in the following format: [auto-increment identifier(ID), start offset (START), end offset (END)]. The files should be named output_<team>_A_<topic>.txt, where <topic> matches with the corresponding input file. The A refers to subtask A applied and team refers to the participant team.


Each output document regarding the Subtask A will contain a line per entry. Each entry represents a span of text that has been identified, i.e. a key phrase. Each entry contains three numbers separated by a single whitespace (or tab). The first number indicates an ID, and should be an auto incremented integer that will be used in later subtasks to reference the corresponding text span. The second number is an integer indicating the START of the text span, that is, the zero-based index with respect to the whole document of the first character of the text span. The third number is an integer that indicates the END of the text phrase, that is, the zero-based index with respect to the whole document of the first character after the span. This means that END minus START is equal to the length in characters of the text span.


For example, the following figure shows the annotation of the previous example (as seen in the Brat annotation tool). In this first subtask it is only important to detect the key phrases, i.e., the fragments of the text that are highlighted in light blue.






This illustration represents the following gold files:
* plain text: input_asma.txt
* key phrases: output_A_asma.txt


The following fragment shows the expected gold output for the previous input file, that exactly matches what the sample figure shows. Notice that the first entry is identified with ID=1 and START=3, END=7. It represents the phrase “asma” seen in the first sentence, which spans from character 3 to character 6 in the document, for a total of 4 characters, hence 7-3=4. The next entry represents the text “afecta”, and so on. It is not necessary for this file to list all text spans in increasing order of START, although doing some might be convenient for manual inspection and easier debugging.


Subtask A  gold output example:
* structured data regarding the key phrases’ offsets:  file output_A_asma.txt


ID        START        END
	1        3        7
2        8        14
3        19        37
4        43        51
5        67        71
6        55        62
7        79        89
8        94        108
9        112        119
10        131        139
11        140        148
12        154        162
13        168        179
14        180        188
15        193        197
16        237        247
17        216        224
18        201        207

NOTE: The headers for the columns in the first line of the example above are only displayed in this document as a visual aid but the actual output files do not contain this first line. The output files start directly with the first numeric entry. The same applies to the rest of the examples shown in this document.


Development evaluation of the Subtask A:
There are different issues to deal with in terms of evaluation when the dev output files are processed. To illustrate some of them the following fragment shows another possible output simulating a participant team (e.g. teamX), but this time some mistakes have been introduced. There are some entries missing while other entries have been incorrectly added, in contrast to the gold file.


example  dev file output_teamX_A_asma.txt
ID        START        END
	1        3        7
3        19        37
4        43        51
5        67        71
7        79        89
8        94        108
9        112        119
10        127        139
11        140        148
12        154        162
13        168        179
14        180        188
16        237        247
17        216        224
15        193        207
19        229        233


The above dev output is equivalent to the following tagged document viewed in Brat:


An evaluation script (i.e. evaluateTasks.py) is provided to help participants to easily detect mistakes. The evaluation script receives two arguments, the path of the gold folder and the path of the dev folder. The gold folder contains the input files (input_<topic>.txt) and the expected gold output files (for Subtask A these are the files called output_A_<topic>.txt), and the dev folder contains the output files to be evaluated (for Subtask A these are the files called output_<team>_A_<topic>.txt).


A set of folders example-gold and example-dev are provided with the previous examples, to illustrate how to use the evaluation script. Running this script using these example folders produces the following result (only showing output that is relevant for the Subtask A):


# Subtask A (detecting phrases)
  input file: ../example-gold/input_asma.txt
  gold file:  ../example-gold/output_A_asma.txt
  dev file:   ../example-dev/output_teamX_A_asma.txt


* Correct (13):
  - "asma" from 3 to 7.
  - "vías respiratorias" from 19 to 37.
  - "pulmones" from 43 to 51.
  - "asma" from 67 to 71.
  - "enfermedad" from 79 to 89.
  - "ataque de asma" from 94 to 108.
  - "produce" from 112 to 119.
  - "empeoran" from 140 to 148.
  - "síntomas" from 154 to 162.
  - "tratamiento" from 168 to 179.
  - "dependen" from 180 to 188.
  - "avanzada" from 216 to 224.
  - "enfermedad" from 237 to 247.
* Partial (2):
  - "síntomas" from 131 to 139 vs "los síntomas" from 127 to 139.
  - "tipo" from 193 to 197 vs "tipo de cáncer" from 193 to 207.
* Missing (3):
  - "afecta" from 8 to 14.
  - "hinchan" from 55 to 62.
  - "cáncer" from 201 to 207.
* Spurious (1):
  - "esté" from 229 to 233.


Precision = 0.93
Recall = 0.82
F1 Score = 0.87
...


The evaluation script reports correct, partial, missing and spurious matches. The expected and actual output files do not need to agree on the ID for each phrase, nor on their order. The evaluator matches are based on the START and END values. However, it is important to pay attention to the IDs which will be used in the following Subtasks B and C.


A brief description about the metrics follows:


* Correct matches are reported when a text in the dev file matches exactly with a corresponding text span in the gold file in START and END values. Only one correct match per entry in the gold file can be matched. Hence, duplicated entries will count as Spurious.
* Partial matches are reported when two intervals [START, END] have a non-empty intersection, such as the case of “síntomas” and “los síntomas” in the previous example. Notice that a partial phrase will only be matched against a single correct phrase. For example, “tipo de cáncer” could be a partial match for both “tipo” and “cáncer”, but it is only counted once as a partial match with the word “tipo”. The word “cancer” is counted then as Missing. This aims to discourage a few large text spans that cover most of the document from getting a very high score.
* Missing matches are those that appear in the gold file but not in the dev file.
* Spurious matches are those that appear in the dev file but not in the gold file.


The evaluation script also reports precision, recall, and a standard F1 measure, calculated as follows:







NOTE: These metrics are only reported for convenience here, to be used by the participants when developing their solutions. The actual score used for ranking participants will be presented later.


A higher precision means that the number of spurious identifications is smaller compared to the number of missing identifications, and a higher recall means the opposite. Partial matches are given half the score of correct matches, while missing and spurious identifications are given no score.
Subtask B: Classification of key phrases


Subtask B continues where Subtask A ends. Given an input file in plain text and the corresponding output from Subtask A, where the text spans that appear in the input have been identified, the purpose of Subtask B is to assign a label to each of these text spans. The labels considered have been inspired by the research in Teleologies, and can be Concept, or Action. The dev output file created per document is named output_<team>_B_<topic>.txt. For example output_teamX_B_asma.txt.


The corresponding gold labels for the example document presented in the previous section are illustrated in the following figure:






This illustration represents the following gold files:
* plain text: input_asma.txt
* key phrases: output_A_asma.txt
* labels: output_B_asma.txt


A brief description of each label follows:
* Concept: the concepts are those key phrases that are able to represent objects and other entities presumed to be of interest for some particular purpose. It is possible to represent simple and complex concepts. Simple concepts just represent singular entries like “asma” or multi-words like “vías respiratorias”, etc. Complex concepts are out scope for this Subtask B because they consist of a group of different concepts.
* Action: is a type of Concept which provokes a modification of another Concept, commonly represented by verbs or phrases that include verbs, but in some cases it can be represented by a non-verb. This Subtask only requires  the recognition of  actions and it is not necessary to detect who performs the action. That problem is tackled in Subtask C.












Continuing with the example from the previous section, the expected output for Subtask B is shown in the following fragment (file example-gold/output_B_asma.txt):


ID     LABEL
	1        Concept
2        Action
3        Concept
4        Concept
6        Action
5        Concept
7        Concept
8        Concept
9        Action
10        Concept
11        Action
12        Concept
13        Concept
14        Action
16        Concept
17        Concept
15        Concept
18        Concept


For each keyphrase identified in Subtask A one of the aforementioned two labels is given. The format is one entry per line, consisting of a number and a label separated by a single whitespace (or tab). The number matches the ID of one of the text spans identified in Subtask A. It is not necessary for the entries to appear in order, since the matching between a LABEL and the corresponding text is done by the ID field.
Development evaluation of the Subtask B
When the evaluation process is carried out the gold and dev files do not need to assign the same ID to the same text spans. When processing the Subtask A, the evaluator builds a mapping between the corresponding IDs in each file, where matching Correct or Partial items are mapped together. The LABEL comparisons in Subtask B will use this mapping to find out the corresponding expected label for each text span.


The following figure is a development example output (ie. output_teamX_B_asma.txt) which includes some incorrect labelling of the  input_asma.txt:






dev file: output_teamX_B_asma.txt
ID     LABEL
	1        Concept
3        Concept
4        Concept
5        Concept
7        Concept
8        Concept
9        Action
10        Concept
11        Action
12        Concept
13        Concept
14        Action
16        Concept
17        Concept
15        Concept
19        Action


The dev output for the evaluation script relevant for this Subtask is the following:


# Subtask A (detecting phrases)
...


# Taks B (labeling phrases)
  input file: ../example-gold/input_asma.txt
  gold file:  ../example-gold/output_B_asma.txt
  dev file:   ../example-dev/output_teamX_B_asma.txt


* Correct (15):
  - Concept "asma" from 3 to 7.
  - Concept "vías respiratorias" from 19 to 37.
  - Concept "pulmones" from 43 to 51.
  - Concept "asma" from 67 to 71.
  - Concept "enfermedad" from 79 to 89.
  - Concept "ataque de asma" from 94 to 108.
  - Action "produce" from 112 to 119.
  - Concept "los síntomas" from 127 to 139.
  - Action "empeoran" from 140 to 148.
  - Concept "síntomas" from 154 to 162.
  - Concept "tratamiento" from 168 to 179.
  - Action "dependen" from 180 to 188.
  - Concept "tipo de cáncer" from 193 to 207.
  - Concept "enfermedad" from 237 to 247.
  - Concept "avanzada" from 216 to 224.


* Incorrect (0):


* Missing (3):
  - Action "afecta" from 8 to 14.
  - Action "hinchan" from 55 to 62.
  - Concept "cáncer" from 201 to 207.


* Spurious (1):
  - Action "esté" from 229 to 233.


Micro-accuracy: 1.00
Macro-accuracy: 0.79
Precision: 0.94
Recall: 0.83
F1: 0.88


...


The script reports Correct, Incorrect, Missing, and Spurious labels:
* Correct are those phrases labelled with the same label in both the gold and the dev files. These are necessarily phrases which were either correctly or partially matched in Subtask A.
* Incorrect are those with different labels. Also, these are phrases which were either correctly or partially matched in Subtask A.
* Missing are the same as in Subtask A.
* Spurious are the same as in Subtask A.


The script reports two different accuracy metrics: micro-accuracy, which considers only the labels involving correct or partial matches in Subtask A; and,  macro-accuracy, which considers all the elements:






Hence, the micro accuracy is always greater or equal to the macro. The micro accuracy is a measure of the quality of Subtask B independent of mistakes made in Subtask A, while the macro accuracy is a measure of the quality of the whole process.


The script also reports standard precision, recall, and F1 computed as follows:









NOTE: These metrics are only reported for convenience here, to be used by the participants when developing their solutions. The actual score used for ranking participants will be presented later.












Subtask C: Setting semantic relationships


Subtask C continues from the output of Subtask B, by linking the entities detected and labelled in each document. Given an input file (i.e. input_<topic>.txt) and the outputs from both Subtasks A and B, the purpose of this Subtask is to recognize all relevant semantic relationships between the entities recognized. The semantic relationships found in the gold example of the previous sections are illustrated in the following figure:


This illustration represents the following gold files:
* plain text: input_asma.txt
* key phrases: output_A_asma.txt
* labels: output_B_asma.txt
* relationships: output_C_asma.txt

In this subtask six types of relationships are defined:
1. Relationships between Concepts:
   * is-a: indicating that the first Concept is a subtype, or more concrete expression of the second Concept. For example “asma” is-a “enfermedad”.
   * part-of: indicating that the first Concept is a constituent part or component of the second Concept, such as in “pulmones” part-of “cuerpo humano”.
   * property-of: indicating that the first Concept defines any property or variable characteristic of the second Concept, such as in “avanzada” property-of “enfermedad”.
   * same-as: for indicating a concept is unambiguously the same as another concept. For example, in “El Instituto Nacional de Alergias y Enfermedades Infecciosas (NIH) apoya las investigaciones destinadas a desarrollar mejores formas de diagnosticar, tratar y prevenir las infecciones, enfermedades del sistema inmune y las alergias.”, the Concept “Instituto Nacional de Alergias y Enfermedades Infecciosas” is the same-as “NIH”.


Notice that these relationships are hinted at by some of the words that appear in the sentence, but it is not necessary for these words to be reported for the purpose of the evaluation. For example, the expression “es una” can be a hint to recognize the relation is-a between “asma” and “enfermedad”, but this fragment of text is not identified or labelled.
1.  Relationships between Actions and Concepts or between Actions themselves:
   * subject: identifies the actor that performs the indicated action. For example in “el asma afecta las vías respiratorias”, the Concept “asma” is who performs the action “afecta”. It can be said a subject plays the producer/actor role by establishing this relationship.
   * target: identifies the actor who receives the effect of the indicated action. For example in “el asma afecta las vías respiratorias”, the Concept “vías respiratorias” receives the effect of the action “afecta”. It can be said a target plays the consumer role by establishing this relationship.


        Notice that Actions can have both a Subject and a Target. However, sometimes the subject of an action is hidden, or non-existent, such as in the case of actions represented by infinitive verbs, for example in the sentence “Diagnosticar el cáncer es difícil”. In this example the Action “Diagnosticar” only has a target, the Concept “cáncer”, since it is not stated who performs the action.
In other examples, the target can either be missing, or be the same as the subject, such as in the case of actions represented by reflexive verbs, for example in the phrase “...los pulmones se hinchan”. In this example the Action “hinchan” has the same subject and target, the Concept “pulmones”. This means that the subject and the target refer both to same concept.
As observed in the gold example, an Action can have more than one Subject and/or Target, if the same fragment of text is used to denote multiple occurrences of said action.
Complex concepts can be represented by tuples, e.g <Subject,Action,Target> (or any variant where target or subject can be missing), in which the Action constitutes its core. Therefore, in Subtask C sometimes the subject or target can be another type of Action, which represents a complex concept. For example, in the sentence “Un ataque de asma se produce cuando los síntomas empeoran”, a complex concept is “síntomas empeoran” where “síntomas” is a Concept and “empeoran” is an Action; “síntomas empeoran” is the act of symptoms getting worse. Therefore, “síntomas empeoran” can be linked to the Action “produce”. The Action “produce” in this sentence is performed by this complex concept, i.e., it is not the symptoms that cause the asthma attack, but rather it is the act of the symptoms getting worse that causes the asthma attack.
The gold output for this Subtask is another file output_C_<topic>.txt with a similar format as before, one entry per line, each entry containing a LABEL,  a SOURCE and a DESTINATION.
The expected output of the previous example is:
LABEL                SOURCE DESTINATION
	is-a                5        7
property-of        17        16
property-of        15        18
subject                2        1
target                2        3
subject                6        4
target                6        4
subject                9        11
target                9        8
target                11        10
subject                14        12
target                14        15
target                14        17
subject                14        13


The label is one of the following linking axioms: is-a, part-of, property-of, subject, target. The SOURCE and DESTINATION are the corresponding ID values  for the key phrases that participate in each relationship. They are defined as follows:
* If the LABEL is one of is-a, part-of, or property-of, then the SOURCE and DESTINATION—two numbers separated by a single whitespace (or tab)—are the source ID and the destination ID of the concepts that participate in the relationship, respectively. For example, in the relationship “asma” is-a “enfermedad”, the source ID is 5, which refers to the key phrase “asma” and the destination ID is 7 which refers to the key phrase “enfermedad” in Subtask A example output.
* If the LABEL is subject or target, then the SOURCE is the ID of the corresponding Action, while DESTINATION is the ID of the corresponding Concept or Action that performs the given role. For example, in the triplet [“asma”, “afecta”, “vías respiratorias”], the Action ID is 2 which points to the text span “afecta”, the Subject ID is 1 which points to the text span “asma” and the Target ID is 3 which points to the text span “vías respiratorias” in Subtask A example output. Hence, for this action we obtain two different entries, one labelled as subject 2 1, and the other as target 2 3.
Development evaluation of the Subtask C
An example of an incorrectly linked document could be as follows:


LABEL                SOURCE DEST
	part-of                1        3
subject                9        11
target                9        8
subject                11        10
subject                14        12
target                14        15
subject                14        13
target                14        17
subject                19        16
target                19        17


Running the evaluation script produces the following output (showing only the relevant part for this Subtask):
# Taks C (linking)
  gold file: ../example-gold/output_C_asma.txt
  dev file:  ../example-dev/output_teamX_C_asma.txt


* Correct (6):
  - subject involving "produce" and "empeoran".
  - target involving "produce" and "ataque de asma".
  - subject involving "dependen" and "síntomas".
  - target involving "dependen" and "tipo".
  - target involving "dependen" and "avanzada".
  - subject involving "dependen" and "tratamiento".
* Missing (8):
  - is-a involving "asma" and "enfermedad".
  - property-of involving "avanzada" and "enfermedad".
  - property-of involving "tipo" and "cáncer".
  - subject involving "afecta" and "asma".
  - target involving "afecta" and "vías respiratorias".
  - subject involving "hinchan" and "pulmones".
  - target involving "hinchan" and "pulmones".
  - target involving "empeoran" and "síntomas".
* Spurious (4):
  - part-of involving "asma" and "vías respiratorias".
  - subject involving "empeoran" and "los síntomas".
  - subject involving "esté" and "enfermedad".
  - target involving "esté" and "avanzada".


Precision: 0.60
Recall: 0.43
F1: 0.50


The script reports the correct, missing and spurious items, defined as follows:
* Correct: relationships that matched exactly to the gold file, including the LABEL and the corresponding IDs for each of the participants.
* Missing: relationships that are in the gold file but not in the dev file, either because the LABEL is wrong, or because one of the IDs didn’t match.
* Spurious: relationships that are in the dev file but not in the gold file, either because the LABEL is wrong, or because one of the IDs didn’t match.






The script also reports standard precision, recall and F1 metrics calculated as follows:









NOTE: These metrics are only reported for convenience here, to be used by the participants when developing their solutions. The actual score used for ranking participants will be presented later.


Corpora description
“MedlinePlus is the National Institutes of Health's Website for patients and their families and friends. Produced by the National Library of Medicine, the world’s largest medical library, it brings you information about diseases, conditions, and wellness issues in language you can understand. MedlinePlus offers reliable, up-to-date health information, anytime, anywhere, for free.” [3] This platform freely provides large health textual data from which we have made a  selection for constituting the eHealth-KD corpus. The selection has been made by sampling specific XML files from the collection available in https://medlineplus.gov/xml.html.


These files contain several entries related to health and medicine topics and have been processed to remove all XML markup to extract the textual content. Only Spanish language items were considered. Once cleaned, each individual item was converted to a plain text document, and some further post-processing is applied to remove unwanted sentences, such as headers, footers and similar elements,  and to flatten HTML lists into plain sentences. The final documents are manually tagged using Brat by a group of annotators. After tagging, a post-processing was applied to Brat’s output files (ANN format) to obtain the output files in the formats described in this document.


The resulting documents and output files are distributed along with the Task. There is no need for participants to download extra data from MedlinePlus servers, since all the input is already distributed.


Test data
The corpus is split into two sets, one is distributed for the purpose of training and development and the remaining documents are kept for blind evaluation (test). This test evaluation set is further divided into three sets, one for each of the evaluation scenarios described below. Of these sets, only input files and the relevant output files are distributed (see below) and the rest of the output files are kept for evaluating participant’s submissions.


After the test evaluation is completed and results are disclosed, the full corpus will be provided, along with all the configuration files and utility scripts used to create the corpus and perform the annotation, to encourage future researchers to build upon these resources.


Additional Resources


Besides the training data, some additional resources will be provided, such as the evaluation script (i.e. evaluateTask.py) and the example files (ie. trial) shown in this document. Participants may use any other external resources as long as they are declared at the time of submission (e.g., WordNet, other corpora, software libraries). However, participants are not allowed to manually annotate the test data prior to submission.
Overall evaluation
There will be three evaluation scenarios:
Scenario 1: Only plain text is given (Subtasks A, B, C).
In this first scenario, the participants will perform the three subtasks consecutively and provide the corresponding development output files. The only input provided are plain text files input_<topic>.txt for a particular list of topics that were not released with the training data.
Systems will be ranked according to an aggregated F1 metric computed on the three tasks, by considering precision and recall as follows:









Besides this aggregated F1 score, individual F1 scores for each of the subtasks will also be reported.


Scenario 2: Plain text and manually annotated key phrase boundaries are given (Subtasks B, C).


In this second scenario participants will perform tasks B and C sequentially, and provide the corresponding output files. As input, they receive both plain text files (input_<topic>.txt), and the corresponding gold files for the task A (output_A_<topic>.txt). The purpose of this scenario is to evaluate the quality of tasks B and C independently from task A. As in the previous scenario, an aggregated F1 metric is reported, based on the following precision and recall:











Besides the aggregated F1 metric, individual scores of F1 for each of the subtasks are also reported.
Scenario 3: Plain text with manually annotated key phrases and their types are given (Subtask C).


In this scenario both the gold outputs for task A and task B are provided, and the participants must only perform the process to obtain task C output files. The purpose of this scenario is to evaluate only the quality of task C independently of the complexity of task A and B. As before, an aggregated F1 metric is reported, based on the following precision and recall:









Submission
Each participant team must submit its results per scenario in a single <team>_<run#>.zip file following this folder structure including in each folder the respective task development outputs:
<scenario1>_/
<subtaskA>/
<subtaskB>/
<subtaskC>/
<scenario2>_/
<subtaskB>/
<subtaskC>/
<scenario3>_/
<subtaskC>/












In addition, it is needed filling a txt filling the following slots:


* Team Name
* Run number
* Summary
* Technique Type: refers the type of techniques used
   * KB: knowledge-based
   * S: Supervised
   * WS: Weak Supervised
   * U: Unsupervised
   * H: Hybrid (in this case other techniques are added using a dash as a separator “-”, ie. H-KB-S)
* Resources
* Authors, Institutions and Emails




Up to three runs (i.e. approaches) can be submitted which can be updated while the evaluation period is active.
An example folder structure for submission will be released.


Discussion group
A Google Group has been set up for this TASS “ eHealth Shared Task” where announcements will be made. Do send your questions and feedback to (tass2018_ehealth-kd@googlegroups.com).


Important dates
12 Feb 2018:
	Trial data ready
	02 Apr 2018:
	Training and development data ready
	06 May 2018:
	Registration deadline due by 23:59 GMT -12:00
	07 May 2018:
	Evaluation start* - Test data released
	21 May 2018:
	Evaluation end* - due by 23:59 GMT -12:00
	28 May 2018:
	Results posted
	02 Jul 2018:
	System description paper submissions due by 23:59 GMT -12:00
	23 Jul 2018:
	Paper reviews due
	31 Jul 2018:
	Author notifications
	31 Aug 2018:
	Camera ready submissions due


Organization committee
Name
	Email
	Institution
	Yoan Gutiérrez Vázquez
	ygutierrez@dlsi.ua.es
	University of Alicante, Spain
	Suilan Estévez Velarde
	sestevez@matcom.uh.cu
	University of Havana, Cuba
	Yudivián Almeida Cruz
	yudy@matcom.uh.cu
	University of Havana, Cuba
	Alejandro Piad Morffis
	apiad@matcom.uh.cu
	University of Havana, Cuba
	Andrés Montoyo Guijarro
	montoyo@dlsi.ua.es
	University of Alicante, Spain
	Rafael Muñoz Guillena
	rafael@dlsi.ua.es
	University of Alicante, Spain
	Contact person
Yoan Gutiérrez Vázquez (ygutierrez@dlsi.ua.es)
Acknowledgments
This research task is partially funded by the University of Alicante, Generalitat Valenciana, Spanish Government (“Ministerio de Economía y Competitividad”)  through the projects REDES (TIN2015-65136- C2-2-R) and “Plataforma inteligente para recuperación, análisis y representación de la información generada por usuarios en Internet” (GRE16-01).
Both projects have partially designed their evaluation stages over this shared task.


References
[1] Gonzalez-Hernandez, G. and Sarker, A. and O’Connor, K. and Savova, G. Capturing the Patient’s Perspective: a Review of Advances in Natural Language Processing of Health-Related Text. Yearbook of medical informatics; 26(01), p 214--227. 2017
[2]   Giunchiglia, F., & Fumagalli, M. (2017, November). Teleologies: Objects, Actions and Functions. In International Conference on Conceptual Modeling (pp. 520-534). Springer, Cham.
[3]   MedlinePlus [Internet]. Bethesda (MD): National Library of Medicine (US). Available from: https://medlineplus.gov/.




________________
[1] http://alt.qcri.org/semeval2017/task10/